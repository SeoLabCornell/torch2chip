model:
  model_type: "meta-llama/Llama-3.1-8B"

dataset:
  name: "wikitext"
  path: "wikitext-2-raw-v1"
  split: "test"

save:
  run_dir: "save/llama3.2-8B_wikitext/"
  logger: "inference.log"

eval:
  chunk_size: 2048
  n_samples: 40

quantization:
  wbit: 8
  abit: 8
  num_samples: 512
  wqtype: smooth_quant
  xqtype: smooth_quant_token

train:
  batch_size: 512

smooth:
  alpha: 0.85

t2c:
  swl: 32
  sfl: 26

export:
  export_samples: 0